# **Лабораторная работа 3.1. Проектирование архитектуры хранилища больших данных**

**Цель:** создать масштабируемую и отказоустойчивую архитектуру хранилища больших данных для студии разработки VR/AR-приложений, предназначенную для комплексного анализа пользовательского опыта, оптимизации процессов рендеринга и стриминга, а также A/B-тестирования игровых механик

**Задача:** обеспечить надежное хранение и консолидацию больших объемов разнородных данных, поступающих из различных источников, реализовать механизмы для эффективной обработки и анализа собранных данных

## **Шаг 1. Определение требований**
### **1.1 Источники данных**
> * VR/AR-гарнитуры – Oculus Quest, HTC Vive, Microsoft HoloLens
> * Внутриигровые логи – логируемые действия, генерируемые самим приложением

### **1.2 Типы данных**
#### **Структурированные данные**

* метаданные A/B-тестов,
* результаты агрегаций

#### **Полуструктурированные данные**

* Сырые события телеметрии (действия, перемещения, взгляд)
> Координаты головы и контроллеров в 3D пространстве (x, y, z), ротация контроллеров и так далее (pitch, yaw, roll). Поступает с частотой 90 Гц и выше.

> Координаты взгляда на экране и их перемещение в течение времени. Две координаты (x, y). Точка фокусировки в 3D пространстве (x, y, z). Диаметр зрачка, вектор направления взгляда. Частота поступления в среднем 90 Гц (60-120 Гц).
* Логи сессий

#### **Неструктурированные данные**

* Скриншоты/записи с играми для углубленного анализа
* Скриншоты/записи моментов багов/ошибок

### **1.3 Объемы данных**

#### **Расчеты по синтетическим данным**

> Трекинг взгляда = 90 Гц * 60 сек * 10 мин * 0.1 КБ = 2.1 МБ

> Трекинг перемещения = 90 Гц * 60 сек * 10 мин * 0.2 КБ = 6.2 МБ

Соответственно, за 10 минут с одного сеанса будет собираться информация на ~ 10 МБ "сырых" данных в день. Предположим, что всего 5 мест, которые могут работать параллельно по 2 человека. Тогда:

> 10 МБ * 5 * 2 = 100 МБ за 10 минут

Для проекта с 1000 тестировщиками в день: ~8.3 ГБ "сырых" телеметрических данных в день

#### **Примерный объем данных**
> Ожидаемый объем: 10-30 ТБ в год.
>
> Рост: 50-100% ежегодно

#### **Скорость поступления:**
> Высокая. Данные с датчиков поступают непрерывным потоком от всех подключенных пользователей в реальном времени.

> * Данные телеметрии с гарнитур (трекинг, действия): в режиме реального времени, пиковая нагрузка до ~50.000 событий в секунду
>
> * Логи сессий (начало/конец, ошибки): пакетная загрузка каждые 5-15 минут

### **1.4 Требования к обработке**
| Тип данных | Обработка |
|----------|----------|
| Валидация гипотез A/B-теста в реальном времени | Непрерывно |
| Распределение трафика, ошибки | В режиме реального времени |
| Детальный анализ результатов теста | Ежедневно |

### **1.5 Доступность данных**
| Критерий | Отклик (сек) |
| ----------------- | ------------ |
| Время отклика для агрегированных аналитических запросов по итогам теста | < 15 |
| Время отклика для сырых данных | < 300 |

**Доступность базы** = ~ 99,95 % (возможен простой не более ~10 часов в год как тех.перерывов)

### **1.6 Безопасность данных**
* Шифрование данных в состоянии покоя и при передаче
* Многофакторная аутентификация для доступа к данным
* Аудит всех действий с данными теста и его конфигурацией
* Соответствие требованиям Федерального закона о обработке и хранении ПДн
* Анонимизация или псевдонимизация ПДн пользователей

## **Шаг 2. Выбор модели хранилища данных**
* Data Lake: хранение необработанных данных в едином репозитории.
* Data Warehouse: хранение структурированных данных, оптимизированных
для аналитики.
* Hybrid Data Storage: сочетание Data Lake и Data Warehouse.

### **2.1 Компоненты архитектуры**
* Слой сбора данных

  * `Apache Kafka` для приема высокоскоростного потока событий телеметрии с гарантированной доставкой
  * `Logstash` для сбора логов

* Слой хранения

  * `HDFS (Hadoop Distributed File System)` для хранения сырых данных
  * `Jino` – сервер для хранения
  * `PostgreSQL` для структурированных данных
  * `MongoDB` для JSON файлов (полуструктурированных данных)

* Слой обработки

  * Потоковая обработка с помощью `Apache Flink` для вычисления ключевых метрик A/B-теста
  * Пакетная обработка с помощью `Apache Spark` для обработки данных по историческим данным

* Слой аналитики

  * `Apache Superset` для визуализации и дашбордов
  * `Jupyter Notebooks` для интерактивной аналитики

* Управление данными

  * `OpenMetadata` для каталогизации всех данных

* Оркестрация и мониторинг

  * `Apache Airflow` для оркестрации ETL, управления зависимостями задач


<!-- ## **Шаг 3. Построение схемы архитектуры**

![Схема архитекруты. Вариант-19](/lab3_var19.jpg) -->


## **Шаг 3. Построение схемы архитектуры**

![Общий вид процесса UML. Вариант-19](/lab3_uml_var19.jpg)


## **Шаг 4. Процесс обработки данных**

### **Сбор данных**
- Сбор потоковых данных происходит с гарнитур (телеметрия, трекинг взгляда, перемещений, событий) --> поступают в `Apache Kafka` с высокой пропускной способностью.
- Пакетные данные (логи сессий, неструктурированные данные) собираются через `Logstash` и загружаются в `Apache Kafka` раз в 10 минут

### **Прием и первичная обработка**
- Поток данных из `Apache Kafka` распределяется на:
  - Потоковый маршрут с помощью `Apache Flink`
    - Сопоставление событий телеметрии
    - Удаление личных данных (или синонимизация)
    - Начальная агрегация данных (среднее время сессии, количество ошибок и так далее)
  - Пакетный маршрут - сырые данные из `Apache Kafka` и пакетные данные из `Logstash` передаются в `HDFS` в изначальном виде

### **Пакетная обработка**
- Анализ A/B-тестов
- Сегментация пользователей
- Прогнозирование оттока пользователей (с помощью ML)
- Преобразование данных из `HDFS` с загрузкой в `PostgreSQL` и `MongoDB`

### **Визуализация и аналитика**
- Использование заранее прописанных скриптов `Python` (на `Jupyter Notebooks`), подключаясь напрямую к `PostgreSQL` и `MongoDB`
- Создание готовых дашбордов и отчетов в `Apache Superset`. Подключение к агрегированным данным в `PostgreSQL`

### **Масштабирование**
Предусмотрено горизонтальное масштабирование благодаря кластерам HDFS, Apache Spark с помощью добавления новых узлов (nodes), что поможет справиться с ростом объема данных

`Apache Kafka` и `Apache Flink` масштабируется количеством партиций (упорядоченная последовательность записей, распределяющая данные по нескольким брокерам)

### **Отказоустойчивость**
- `HDFS` обеспечивает отказоустойчивость за счет репликации блоков данных (при потере нод, система не прервется)
- Регулярное резервное копирование ключевых метаданных и конфигураций
- `Apache Kafka` реплицирует партиции между брокерами

## **Безопасность**
* Шифрование
  * `Jino` автоматически шифрует все файлы, а на `PostgreSQL` также ставит дополнительную защиту
  * Данные в `HDFS` шифруются с использованием `HDFS Transparent Encryption`
  * `SSL`/`TLS` защита при передаче между компонентами системы

* Аутентификация и авторизация
  * Детальный контроль доступа к данным (базы данных, таблицы в `HDFS`, `PostgreSQL` и `MongoDB`) реализуется с помощью `Apache Ranger`. Доступ настраивается в соответствии с ролями каждого пользователя (с помощью профилей доступа)

* `Apache Ranger` ведет детальные логи всех операций доступа к данным
* Реализация многофакторной аутентификации (MFA/2FA/3FA) на всех точках входа в систему


<!-- ## **Общий процесс в UML**


![Общий вид процесса UML. Вариант-19](/lab3_uml_var19.jpg) -->
